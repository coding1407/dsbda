# -*- coding: utf-8 -*-
"""F_GRPA7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QxzjksxkxjIUNBYEKMpwKEF6eo4MnKky

DSBDA LAB

GROUP A - 07

DATA ANALYTICS(TEXT)
"""

import numpy as np
import pandas as pd
import nltk as nlt
import warnings

text="""What is machine learning?
Machine learning is a branch of artificial intelligence (AI) and computer science which focuses on the use of data and algorithms to imitate the way that humans learn, gradually improving its accuracy.
IBM has a rich history with machine learning. One of its own, Arthur Samuel, is credited for coining the term, “machine learning” with his research (PDF, 481 KB) (link resides outside IBM) around the game of checkers. Robert Nealey, the self-proclaimed checkers master, played the game on an IBM 7094 computer in 1962, and he lost to the computer. Compared to what can be done today, this feat seems trivial, but it’s considered a major milestone in the field of artificial intelligence."""

text

from nltk import word_tokenize,sent_tokenize

nlt.download('punkt')

word_tokenize(text)

sent_tokenize(text)

"""#Removing Stop-Words"""

from nltk.corpus import stopwords

import nltk

nltk.download('stopwords')

stop_words=stopwords.words('english')

print(stop_words)

token=word_tokenize(text)
cleaned_token=[]
for word in token:
  if word not in stop_words:
    cleaned_token.append(word)

print("this is the unclean version:",token)    
print("this is the clean version:",cleaned_token)

words= [token.lower()
        for token in cleaned_token if token.isalpha()
        ]

"""#Stemming"""

from nltk.stem import PorterStemmer

stemmer=PorterStemmer()

port_stemmer_output= [
  stemmer.stem(word) for word in words]

print(port_stemmer_output)

"""#Lemmatization"""

from nltk.stem import WordNetLemmatizer

nltk.download('wordnet')

lemmatizer = WordNetLemmatizer()

lemmatizer_output =[
    lemmatizer.lemmatize(word) for word in words]

print(lemmatizer_output)

"""#Part Of Speech Tagging (POS)"""

from nltk import pos_tag

nltk.download('averaged_perceptron_tagger')

tagged=pos_tag(lemmatizer_output)

tagged

"""afternoon session """

from sklearn.feature_extraction.text import TfidfVectorizer

from sklearn.metrics.pairwise 
#import cosine_similarity not needed to be added so skipped

docs=["Sachin is considered to be one of the greatest cricket players",
      "federer is considered one of the greatest tennis players",
      "Nadal is considered one of the best tennis player",
      "virat is the captain of the indian cricket team"]

vectorizer = TfidfVectorizer(analyzer="word",norm=None)
#vectorizer is the object of class
Mat=vectorizer.fit(docs)
#mat is the matrix which carries unique words
print(Mat.vocabulary_)
#vocabulary will print the unique words in the doc

tfidfMat = vectorizer.fit_transform(docs)
print(tfidfMat)

features_names = vectorizer.get_feature_names_out()
print(features_names)

dense = tfidfMat.todense()
denselist = dense.tolist()
df = pd.DataFrame(denselist,columns = features_names)
df

features_names = sorted(vectorizer.get_feature_names_out())
features_names

docList = ['Doc1','Doc2','Doc3','Doc4']
s=pd.DataFrame(tfidfMat.todense(),
               index=sorted(docList),columns=features_names)
print(s)

"""#Compute Cosine Similarity(optional)
csim = cosine_similarity(tfidfMat,tfidfMat)

csimDf = pd.DataFrame(csim,index = sorted(docList),

columns = sorted(docList))

print(csimDf)
"""

